{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from random import randint\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = 'Data\\train' #path to train dataset\n",
    "image_list = []\n",
    "labels_list = []\n",
    "for root, dirs, files in os.walk(main_folder_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
    "            try:\n",
    "                img_path = os.path.join(root, filename)\n",
    "                label = os.path.basename(root)  \n",
    "                labels_list.append(label)\n",
    "                img = Image.open(img_path)\n",
    "                img_array = np.array(img)\n",
    "                image_list.append(img_array)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "def process_images(image_list, label_list):\n",
    "    processed_images = []\n",
    "    processed_labels = []\n",
    "    for img, label in zip(image_list, label_list):\n",
    "        img_pil = Image.fromarray(img).convert('L')\n",
    "        img_resized = img_pil.resize((128, 128))\n",
    "        processed_images.append(np.array(img_resized)) \n",
    "        processed_labels.append(label)\n",
    "    stacked_images = np.stack(processed_images, axis=0)\n",
    "    return stacked_images, processed_labels\n",
    "\n",
    "images_list, labels = process_images(image_list, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image(symbol,image_list=images_list,labels_list=labels_list):\n",
    "    symbol_map = {\n",
    "    '.': 'decimal',\n",
    "    '/': 'div',\n",
    "    '8': 'eight',\n",
    "    '=': 'equal',\n",
    "    '5': 'five',\n",
    "    '4': 'four',\n",
    "    '-': 'minus',\n",
    "    '9': 'nine',\n",
    "    '1': 'one',\n",
    "    '+': 'plus cleaned',\n",
    "    '7': 'seven',\n",
    "    '6': 'six',\n",
    "    '3': 'three',\n",
    "    '*': 'times',\n",
    "    '2': 'two',\n",
    "    '0': 'zero'\n",
    "    }\n",
    "    mask=np.all(np.array([labels_list])==symbol_map[symbol],axis=0)\n",
    "    return image_list[mask][randint(0,sum(mask)-1)]\n",
    "\n",
    "def extract_mser_segments(image, resize_shape=(128, 128), padding_factor=2.2, extra_padding=50,show_plots:bool=False): # 2.2 50\n",
    "    mser = cv2.MSER_create()\n",
    "    regions, _ = mser.detectRegions(image)\n",
    "    image_color = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    bounding_boxes = []\n",
    "    \n",
    "    for p in regions:\n",
    "        x, y, w, h = cv2.boundingRect(p.reshape(-1, 1, 2))\n",
    "        if w > 5 and h > 5:\n",
    "            bounding_boxes.append((x, y, w, h))\n",
    "\n",
    "    def merge_bounding_boxes(boxes, overlap_thresh=0.3, vertical_merge_thresh=20):\n",
    "        merged_boxes = []\n",
    "        used = [False] * len(boxes)\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            if used[i]:\n",
    "                continue\n",
    "\n",
    "            x1, y1, w1, h1 = boxes[i]\n",
    "            merged_box = [x1, y1, x1 + w1, y1 + h1]\n",
    "\n",
    "            for j in range(i + 1, len(boxes)):\n",
    "                if used[j]:\n",
    "                    continue\n",
    "\n",
    "                x2, y2, w2, h2 = boxes[j]\n",
    "\n",
    "                if (x1 < x2 + w2 and x1 + w1 > x2 and y1 < y2 + h2 and y1 + h1 > y2) or \\\n",
    "                   (abs(x1 - x2) < 10 and abs(w1 - w2) < 10 and abs(y1 + h1 - y2) < vertical_merge_thresh):\n",
    "                    merged_box[0] = min(merged_box[0], x2)\n",
    "                    merged_box[1] = min(merged_box[1], y2)\n",
    "                    merged_box[2] = max(merged_box[2], x2 + w2)\n",
    "                    merged_box[3] = max(merged_box[3], y2 + h2)\n",
    "                    used[j] = True\n",
    "\n",
    "            merged_boxes.append((merged_box[0], merged_box[1], merged_box[2] - merged_box[0], merged_box[3] - merged_box[1]))\n",
    "            used[i] = True\n",
    "\n",
    "        return merged_boxes\n",
    "\n",
    "    merged_boxes = merge_bounding_boxes(bounding_boxes)\n",
    "    merged_boxes = sorted(merged_boxes, key=lambda b: b[0])\n",
    "    resized_segments = []\n",
    "\n",
    "    for index, (x, y, w, h) in enumerate(merged_boxes, start=1):\n",
    "        cv2.rectangle(image_color, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        segment = image[y:y+h, x:x+w]\n",
    "        height, width = segment.shape\n",
    "        \n",
    "        max_dim = int(max(height, width) * padding_factor)\n",
    "        padded_segment = np.full((max_dim, max_dim), 255, dtype=np.uint8)\n",
    "        \n",
    "        if height > width:\n",
    "            new_width = int((max_dim / height) * width)\n",
    "            resized_segment = cv2.resize(segment, (new_width, max_dim))\n",
    "            x_offset = (max_dim - new_width) // 2\n",
    "            padded_segment[:, x_offset:x_offset + new_width] = resized_segment\n",
    "        else:\n",
    "            new_height = int((max_dim / width) * height)\n",
    "            resized_segment = cv2.resize(segment, (max_dim, new_height))\n",
    "            y_offset = (max_dim - new_height) // 2\n",
    "            padded_segment[y_offset:y_offset + new_height, :] = resized_segment    \n",
    "        \n",
    "        extra_padded_segment = np.full((max_dim + 2 * extra_padding, max_dim + 2 * extra_padding), 255, dtype=np.uint8)\n",
    "        extra_padded_segment[extra_padding:max_dim + extra_padding, extra_padding:max_dim + extra_padding] = padded_segment\n",
    "    \n",
    "        final_segment = cv2.resize(extra_padded_segment, resize_shape)\n",
    "        resized_segments.append(final_segment)\n",
    "    return resized_segments\n",
    "\n",
    "def threshold_image(image: np.ndarray):\n",
    "    return (image > 240).astype('uint8') * 255\n",
    "\n",
    "def cutspace(image: np.ndarray, left: bool = False, right: bool = False):\n",
    "    if not left and not right:\n",
    "        return image\n",
    "    \n",
    "    rows, cols = image.shape\n",
    "    left_cutoff = 0\n",
    "    right_cutoff = cols\n",
    "\n",
    "    if left:\n",
    "        for i in range(cols):\n",
    "            if not np.all(image[:, i] == 255):\n",
    "                left_cutoff = i//2\n",
    "                break\n",
    "\n",
    "    if right:\n",
    "        for i in range(cols - 1, -1, -1):\n",
    "            if not np.all(image[:, i] == 255):\n",
    "                right_cutoff = (i + 1+cols)//2\n",
    "                break\n",
    "\n",
    "    return image[:, left_cutoff:right_cutoff]\n",
    "\n",
    "def get_eqn(eqn: str):\n",
    "    eqn_images = []\n",
    "    for i, symbol in enumerate(eqn):\n",
    "        image = threshold_image(get_random_image(symbol))\n",
    "        if symbol.isdigit():\n",
    "            left = i != 0 and eqn[i - 1].isdigit()\n",
    "            right = i != len(eqn) - 1 and eqn[i + 1].isdigit()\n",
    "            image = cutspace(image, left, right)\n",
    "        eqn_images.append(image)\n",
    "    \n",
    "    output = np.hstack(eqn_images)\n",
    "    return output\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_equation(equation_str):\n",
    "    equation_str = equation_str.replace(\" \", \"\")\n",
    "    pattern = r\"(\\d+\\.?\\d*)([\\+\\-\\*/])(\\d+\\.?\\d*)\"\n",
    "    match = re.match(pattern, equation_str)\n",
    "    if not match:\n",
    "        raise ValueError(\"Invalid equation format. Expected format like '12.5 + 3.4 ='\")\n",
    "    num1 = float(match.group(1))  \n",
    "    operator = match.group(2)\n",
    "    num2 = float(match.group(3)) \n",
    "\n",
    "    if operator == \"+\":\n",
    "        result = num1 + num2\n",
    "    elif operator == \"-\":\n",
    "        result = num1 - num2\n",
    "    elif operator == \"*\":\n",
    "        result = num1 * num2\n",
    "    elif operator == \"/\":\n",
    "        if num2 == 0:\n",
    "            raise ZeroDivisionError(\"Division by zero is not allowed.\")\n",
    "        result = num1 / num2\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported operator. Supported operators are: +, -, *, /\")\n",
    "\n",
    "    return int(result) if result.is_integer() else result\n",
    "\n",
    "def get_number(n:int):\n",
    "    return randint(10**(n-1),10**(n)-1)\n",
    "def generate_equation():\n",
    "    operator_choices=[\"+\",\"-\",\"*\"]\n",
    "    order_1=randint(1,3)\n",
    "    order_2=randint(1,3)\n",
    "    num1=str(get_number(order_1))\n",
    "    num2=str(get_number(order_2))\n",
    "    return num1+operator_choices[randint(0,2)]+num2\n",
    "\n",
    "def generate_final():\n",
    "    eqn=generate_equation()\n",
    "    output=parse_equation(eqn)\n",
    "    image=get_eqn(eqn)\n",
    "    return image,output\n",
    "\n",
    "def get_image_and_labels():\n",
    "    eqn=generate_equation()\n",
    "    image=get_eqn(eqn)\n",
    "    segments=extract_mser_segments(image)\n",
    "    if len(segments)==len(eqn):\n",
    "        return segments,list(eqn)\n",
    "    else:\n",
    "        return None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "* If the model is directly applied to predict segmented output it will fail as the segmented output does not have any padding\n",
    "* To prevent this problem, random equations are generated, corresponding images are picked and horizonatally stacked with reduced spacing between digits to simulate handwritting\n",
    "* Then created a dataset of approximately 10,000 which are images taken after segmenting and padding the randomly generated equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images_list=[]\n",
    "new_labels_list=[]\n",
    "while len(new_images_list)<10000:\n",
    "    img_list,lbl_list=get_image_and_labels()\n",
    "    if img_list:\n",
    "        new_images_list.extend(img_list)\n",
    "        new_labels_list.extend(lbl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list=new_images_list\n",
    "labels=new_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7640\\163483072.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  image_array = torch.tensor(images_list, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 16\n",
    "image_array = torch.tensor(images_list, dtype=torch.float32)\n",
    "image_tensor = image_array.unsqueeze(1) / 255.0  \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)  \n",
    "labels_tensor = torch.tensor(labels_encoded, dtype=torch.long)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model\n",
    "* added regularization as previous simple CNN models ended up overfitting within 10 epochs with the same hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=16):\n",
    "        super(RegularizedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)  \n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(16)  \n",
    "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = x.view(-1, 32 * 32 * 32)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1) \n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = RegularizedCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(image_tensor, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0784, Accuracy: 66.86%\n",
      "Epoch [2/10], Loss: 0.4854, Accuracy: 83.31%\n",
      "Epoch [3/10], Loss: 0.3769, Accuracy: 86.71%\n",
      "Epoch [4/10], Loss: 0.3015, Accuracy: 89.09%\n",
      "Epoch [5/10], Loss: 0.2880, Accuracy: 90.32%\n",
      "Epoch [6/10], Loss: 0.2421, Accuracy: 91.40%\n",
      "Epoch [7/10], Loss: 0.2141, Accuracy: 92.32%\n",
      "Epoch [8/10], Loss: 0.1933, Accuracy: 93.11%\n",
      "Epoch [9/10], Loss: 0.1827, Accuracy: 93.40%\n",
      "Epoch [10/10], Loss: 0.1728, Accuracy: 93.81%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'reg_cnn_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "with open(\"LabelEncoder.pkl\", 'wb') as f:\n",
    "    pkl.dump(label_encoder, f)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           *       0.99      1.00      1.00       680\n",
      "           +       1.00      1.00      1.00       616\n",
      "           -       1.00      1.00      1.00       696\n",
      "           0       1.00      0.98      0.99       390\n",
      "           1       1.00      0.99      1.00       827\n",
      "           2       0.99      1.00      0.99       865\n",
      "           3       1.00      1.00      1.00       848\n",
      "           4       1.00      1.00      1.00       883\n",
      "           5       1.00      1.00      1.00       830\n",
      "           6       0.99      1.00      0.99       836\n",
      "           7       0.99      1.00      1.00       827\n",
      "           8       1.00      0.99      1.00       838\n",
      "           9       1.00      1.00      1.00       867\n",
      "\n",
      "    accuracy                           1.00     10003\n",
      "   macro avg       1.00      1.00      1.00     10003\n",
      "weighted avg       1.00      1.00      1.00     10003\n",
      "\n",
      "Overall Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=label_encoder.classes_))\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
